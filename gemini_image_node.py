import base64
import configparser
import io
import json
import threading
from pathlib import Path
from typing import List, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

import numpy as np
import requests
import torch
import urllib3
from PIL import Image, ImageOps

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# çº¿ç¨‹æœ¬åœ°å­˜å‚¨,ç”¨äº Session å¤ç”¨
thread_local = threading.local()

CATEGORY = "artsmcp"
CONFIG_SECTION = "Gemini-banana"  # ç‹¬ç«‹é…ç½®èŠ‚
CONFIG_PATH = Path(__file__).parent / "config.ini"
CONFIG = configparser.ConfigParser()
if CONFIG_PATH.exists():
    CONFIG.read(CONFIG_PATH, encoding="utf-8")
else:
    CONFIG[CONFIG_SECTION] = {}  # ä½¿ç”¨ç‹¬ç«‹é…ç½®èŠ‚
    with CONFIG_PATH.open("w", encoding="utf-8") as fp:
        CONFIG.write(fp)

# å›¾åƒå°ºå¯¸æ˜ å°„ï¼ˆGemini æ”¯æŒå¤šç§å®½é«˜æ¯”ï¼‰
IMAGE_SIZE_MAP = {
    "1:1": "1:1",
    "2:3": "2:3",
    "3:2": "3:2",
    "3:4": "3:4",
    "4:3": "4:3",
    "4:5": "4:5",
    "5:4": "5:4",
    "9:16": "9:16",
    "16:9": "16:9",
    "21:9": "21:9",
}

# æ¨¡å‹æ˜ å°„
MODEL_MAP = {
    "gemini-3-pro-image-preview": "gemini-3-pro-image-preview",
    "gemini-3-pro-image-preview-2k": "gemini-3-pro-image-preview-2k",
    "gemini-3-pro-image-preview-4k": "gemini-3-pro-image-preview-4k",
}

# å“åº”æ ¼å¼æ˜ å°„
RESPONSE_FORMAT_MAP = {
    "URL": "url",
    "Base64": "b64_json",
}


def tensor_to_base64(image_tensor):
    """å°† ComfyUI tensor è½¬æ¢ä¸º base64 å­—ç¬¦ä¸²"""
    if len(image_tensor.shape) > 3:
        image_tensor = image_tensor[0]
    
    array = np.clip(image_tensor.cpu().numpy() * 255.0, 0, 255).astype(np.uint8)
    pil_image = Image.fromarray(array, mode='RGB')
    
    buffered = io.BytesIO()
    pil_image.save(buffered, format="JPEG", quality=95)
    img_bytes = buffered.getvalue()
    base64_string = base64.b64encode(img_bytes).decode('utf-8')
    
    return f"data:image/jpeg;base64,{base64_string}"


def get_session():
    """è·å–çº¿ç¨‹æœ¬åœ°çš„ Session (å¤ç”¨è¿æ¥æ± ,ä½¿ç”¨å®˜æ–¹æ¨èçš„ HTTPAdapter é…ç½®)"""
    if not hasattr(thread_local, "session"):
        # åˆ›å»º Session
        session = requests.Session()
        
        # ä½¿ç”¨ HTTPAdapter ç²¾ç»†æ§åˆ¶è¿æ¥æ± 
        adapter = requests.adapters.HTTPAdapter(
            pool_connections=10,  # è¿æ¥æ± æ•°é‡
            pool_maxsize=10,      # æ¯ä¸ªè¿æ¥æ± çš„æœ€å¤§è¿æ¥æ•°
            max_retries=0         # é‡è¯•ç”±ä¸Šå±‚ make_api_request æ§åˆ¶
        )
        
        # æŒ‚è½½åˆ° http å’Œ https
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        # å­˜å‚¨åˆ°çº¿ç¨‹æœ¬åœ°
        thread_local.session = session
    
    return thread_local.session


def download_image_to_tensor(url: str, timeout: int = 60):
    """ä» URL ä¸‹è½½å›¾ç‰‡å¹¶è½¬æ¢ä¸º tensor"""
    response = None
    
    try:
        print(f"[INFO] æ­£åœ¨ä¸‹è½½å›¾ç‰‡: {url}")
        
        # ä½¿ç”¨çº¿ç¨‹æœ¬åœ° Session (è¿æ¥æ± å¤ç”¨)
        session = get_session()
        response = session.get(url, timeout=timeout, verify=False, stream=True)
        response.raise_for_status()
        
        pil_image = Image.open(io.BytesIO(response.content)).convert('RGB')
        print(f"[INFO] å›¾ç‰‡å°ºå¯¸: {pil_image.size}")
        
        numpy_image = np.array(pil_image).astype(np.float32) / 255.0
        tensor = torch.from_numpy(numpy_image)
        
        return tensor
        
    except Exception as e:
        print(f"[ERROR] ä¸‹è½½å›¾ç‰‡å¤±è´¥: {e}")
        return None
        
    finally:
        # æ¸…ç†èµ„æº (ä½†ä¿ç•™ Session ä¾›çº¿ç¨‹å¤ç”¨)
        try:
            if response is not None:
                response.close()
        except Exception as e:
            print(f"[WARN] æ¸…ç†ä¸‹è½½è¿æ¥å¤±è´¥: {e}")


def base64_to_tensor(b64_string: str):
    """å°† base64 å­—ç¬¦ä¸²è½¬æ¢ä¸º tensor (æ”¯æŒ data URI æ ¼å¼)"""
    try:
        # å¤„ç† data URI æ ¼å¼ (å¦‚: data:image/png;base64,...)
        if b64_string.startswith("data:image"):
            # æå–å®é™…çš„ base64 æ•°æ®éƒ¨åˆ†
            b64_string = b64_string.split(",", 1)[1]
        
        img_bytes = base64.b64decode(b64_string)
        pil_image = Image.open(io.BytesIO(img_bytes)).convert('RGB')
        numpy_image = np.array(pil_image).astype(np.float32) / 255.0
        tensor = torch.from_numpy(numpy_image)
        return tensor
    except Exception as e:
        print(f"[ERROR] Base64 è½¬æ¢å¤±è´¥: {e}")
        return None


def make_api_request(url: str, headers: dict, payload: dict, timeout: int = 120, max_retries: int = 3, backoff: int = 2):
    """å‘é€ API è¯·æ±‚ï¼ˆæ”¯æŒé‡è¯•ï¼‰"""
    import time
    
    # æ‰“å°è¯·æ±‚ä¿¡æ¯
    print(f"[INFO] å‘é€è¯·æ±‚åˆ°: {url}")
    print(f"[INFO] è¯·æ±‚å‚æ•°: {json.dumps(payload, ensure_ascii=False)[:200]}...")
    
    last_error = None
    response = None
    
    for attempt in range(1, max_retries + 1):
        try:
            # æ¸…ç†ä¸Šæ¬¡é‡è¯•çš„ response
            if response is not None:
                response.close()
                response = None
            
            if attempt > 1:
                wait_time = min(backoff ** (attempt - 1), 20)  # æŒ‡æ•°é€€é¿: 2s, 4s, 8s, æœ€å¤§20s
                print(f"[INFO] ç¬¬ {attempt} æ¬¡é‡è¯•ï¼Œç­‰å¾… {wait_time} ç§’...")
                time.sleep(wait_time)
            
            # ä½¿ç”¨çº¿ç¨‹æœ¬åœ° Session (è¿æ¥æ± å¤ç”¨)
            session = get_session()
            response = session.post(
                url,
                headers=headers,
                json=payload,
                timeout=timeout,
                verify=False
            )
            
            # è¯¦ç»†çš„çŠ¶æ€ç å¤„ç†
            print(f"[INFO] HTTP çŠ¶æ€ç : {response.status_code}")
            
            # æ£€æŸ¥æ˜¯å¦æˆåŠŸ
            response.raise_for_status()
            
            result = response.json()
            print(f"[SUCCESS] è¯·æ±‚æˆåŠŸï¼")
            print(f"[DEBUG] å®Œæ•´å“åº”æ•°æ®: {json.dumps(result, ensure_ascii=False, indent=2)}")
            
            # æˆåŠŸåå…³é—­ response (ä½†ä¿ç•™ Session)
            response.close()
            return result
            
        except requests.exceptions.HTTPError as exc:
            last_error = exc
            print(f"[ERROR] HTTP é”™è¯¯ (å°è¯• {attempt}/{max_retries}): {exc}")
            
            # æ‰“å°å“åº”å†…å®¹ç”¨äºè°ƒè¯•
            try:
                if response is not None:
                    error_detail = response.json()
                    print(f"[ERROR] é”™è¯¯è¯¦æƒ…: {json.dumps(error_detail, ensure_ascii=False)}")
            except:
                if response is not None:
                    print(f"[ERROR] å“åº”æ–‡æœ¬: {response.text[:500]}")
            
            # 4xx å®¢æˆ·ç«¯é”™è¯¯ç›´æ¥æŠ›å‡ºï¼Œä¸é‡è¯•ï¼ˆé™¤äº† 429 é™æµï¼‰
            if 400 <= exc.response.status_code < 500 and exc.response.status_code != 429:
                print(f"[ERROR] å®¢æˆ·ç«¯é”™è¯¯ ({exc.response.status_code})ï¼Œä¸è¿›è¡Œé‡è¯•")
                # æ¸…ç†èµ„æº
                if response:
                    response.close()
                raise
                
        except requests.exceptions.Timeout as exc:
            last_error = exc
            print(f"[ERROR] è¯·æ±‚è¶…æ—¶ (å°è¯• {attempt}/{max_retries}): {exc}")
            print(f"[DEBUG] è¶…æ—¶ç±»å‹: {type(exc).__name__}")
            
        except requests.exceptions.ConnectionError as exc:
            last_error = exc
            print(f"[ERROR] è¿æ¥å¤±è´¥ (å°è¯• {attempt}/{max_retries}): {exc}")
            
        except Exception as exc:
            last_error = exc
            print(f"[ERROR] æœªçŸ¥é”™è¯¯ (å°è¯• {attempt}/{max_retries}): {exc}")
        
        finally:
            # ç¡®ä¿ response è¢«å…³é—­ (ä½†ä¿ç•™çº¿ç¨‹æœ¬åœ° Session)
            try:
                if response is not None:
                    response.close()
            except Exception as e:
                print(f"[WARN] æ¸…ç†è¿æ¥å¤±è´¥: {e}")
        
        # å¦‚æœè¿˜æœ‰é‡è¯•æœºä¼šï¼Œç»§ç»­å¾ªç¯
        if attempt < max_retries:
            continue
    
    # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
    print(f"\n[ERROR] âŒ è¯·æ±‚æœ€ç»ˆå¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡")
    print(f"[ERROR] æœ€åé”™è¯¯: {last_error}")
    print(f"\nğŸ’¡ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
    print(f"   1. æ£€æŸ¥ API æœåŠ¡æ˜¯å¦æ­£å¸¸: {url}")
    print(f"   2. ç¡®è®¤ API Key æ˜¯å¦æœ‰æ•ˆ")
    print(f"   3. ç¨åå†è¯•ï¼Œå¯èƒ½æ˜¯æœåŠ¡å™¨ä¸´æ—¶è¿‡è½½")
    print(f"   4. æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦ç¨³å®š")
    print(f"   5. å°è¯•å¢åŠ  timeout å‚æ•°å€¼")
    
    if last_error:
        raise last_error
    raise RuntimeError("æœªçŸ¥è¯·æ±‚å¤±è´¥")


class GeminiBananaNode:
    """Gemini Banana å›¾ç‰‡ç”ŸæˆèŠ‚ç‚¹ - æ”¯æŒæ–‡ç”Ÿå›¾ã€å›¾ç”Ÿå›¾ã€å¤šå›¾èåˆ"""
    
    def __init__(self):
        self.verbose = False  # é»˜è®¤å…³é—­è¯¦ç»†æ—¥å¿—
    
    def log(self, message, level="INFO"):
        """ç»Ÿä¸€æ—¥å¿—è¾“å‡º (æ”¯æŒåˆ†çº§)"""
        if level == "DEBUG" and not self.verbose:
            return  # DEBUG æ—¥å¿—åªåœ¨ verbose æ¨¡å¼ä¸‹æ‰“å°
        print(message)
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "æç¤ºè¯": ("STRING", {
                    "multiline": True,
                    "default": "æ˜Ÿé™…ç©¿è¶Š,é»‘æ´,ç”µå½±å¤§ç‰‡,è¶…ç°å®ä¸»ä¹‰",
                    "display": "input"
                }),
                "APIå¯†é’¥": ("STRING", {
                    "multiline": False,
                    "default": CONFIG.get(CONFIG_SECTION, "api_key", fallback="")
                }),
                "APIåœ°å€": ("STRING", {
                    "multiline": False,
                    "default": CONFIG.get(CONFIG_SECTION, "api_url", fallback="https://api.openai.com")
                }),
                "æ¨¡å‹": (list(MODEL_MAP.keys()), {
                    "default": list(MODEL_MAP.keys())[0]
                }),
                "å®½é«˜æ¯”": (list(IMAGE_SIZE_MAP.keys()), {
                    "default": "1:1"
                }),
                "å“åº”æ ¼å¼": (list(RESPONSE_FORMAT_MAP.keys()), {
                    "default": "URL"
                }),
                "è¶…æ—¶ç§’æ•°": ("INT", {
                    "default": 120,
                    "min": 30,
                    "max": 600,
                    "step": 10
                }),
                "æœ€å¤§é‡è¯•æ¬¡æ•°": ("INT", {
                    "default": 3,
                    "min": 1,
                    "max": 10,
                    "step": 1
                }),
                "å¹¶å‘è¯·æ±‚æ•°": ("INT", {
                    "default": 1,
                    "min": 1,
                    "max": 10,
                    "step": 1
                }),
                "å¯ç”¨åˆ†è¡Œæç¤ºè¯": ("BOOLEAN", {
                    "default": False
                }),
                "åŒ¹é…å‚è€ƒå°ºå¯¸": ("BOOLEAN", {
                    "default": False,
                    "label_on": "å¼€å¯",
                    "label_off": "å…³é—­"
                }),
                "è¯¦ç»†æ—¥å¿—": ("BOOLEAN", {
                    "default": False
                }),
            },
            "optional": {
                "å‚è€ƒå›¾ç‰‡1": ("IMAGE", {}),
                "å‚è€ƒå›¾ç‰‡2": ("IMAGE", {}),
                "å‚è€ƒå›¾ç‰‡3": ("IMAGE", {}),
                "å‚è€ƒå›¾ç‰‡4": ("IMAGE", {}),
            }
        }
    
    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("å›¾ç‰‡è¾“å‡º",)
    FUNCTION = "generate_image"
    CATEGORY = CATEGORY
    OUTPUT_NODE = False  # æ ‡æ˜è¿™ä¸æ˜¯è¾“å‡ºèŠ‚ç‚¹
    
    @classmethod
    def IS_CHANGED(cls, **kwargs):
        """å¼ºåˆ¶æ¯æ¬¡éƒ½é‡æ–°æ‰§è¡Œ,ä¸ä½¿ç”¨ç¼“å­˜(å› ä¸ºæ˜¯å¤–éƒ¨APIè¯·æ±‚)"""
        import time
        return time.time()
    
    def _prepare_prompts(self, prompt: str, enable_multiline: bool, concurrent_requests: int):
        """å‡†å¤‡æç¤ºè¯åˆ—è¡¨"""
        prompts = []
        if enable_multiline:
            prompts = [line.strip() for line in prompt.split('\n') if line.strip()]
            print("\n" + "="*60)
            print("[Gemini-Banana] å¯ç”¨åˆ†è¡Œæç¤ºè¯æ¨¡å¼")
            print(f"  - æ‹†åˆ†åæç¤ºè¯æ•°é‡: {len(prompts)}")
            print(f"  - æ¯è¡Œå¹¶å‘è¯·æ±‚æ•°: {concurrent_requests}")
            print(f"  - æ€»ä»»åŠ¡æ•°: {len(prompts)} è¡Œ Ã— {concurrent_requests} å¹¶å‘ = {len(prompts) * concurrent_requests} å¼ å›¾")
            for idx, p in enumerate(prompts, 1):
                print(f"  - æç¤ºè¯{idx}: {p[:50]}...")
            print("="*60 + "\n")
        else:
            prompts = [prompt]
            print("\n" + "="*60)
            print("[Gemini-Banana] å®Œæ•´æç¤ºè¯æ¨¡å¼")
            print(f"  - æç¤ºè¯: {prompt[:50]}...")
            print(f"  - å¹¶å‘è¯·æ±‚æ•°: {concurrent_requests}")
            print(f"  - æ€»ä»»åŠ¡æ•°: {concurrent_requests} å¼ å›¾")
            print("="*60 + "\n")
        return prompts
    
    def _prepare_input_images(self, image1, image2, image3, image4):
        """æ”¶é›†å’Œè½¬æ¢è¾“å…¥å›¾ç‰‡"""
        input_images = []
        for img in [image1, image2, image3, image4]:
            if img is not None:
                input_images.append(img)
        
        image_urls = []
        if input_images:
            for idx, img_tensor in enumerate(input_images):
                base64_url = tensor_to_base64(img_tensor)
                image_urls.append(base64_url)
                print(f"[INFO] å·²è½¬æ¢å›¾ç‰‡{idx + 1}ä¸º Base64")
        
        return image_urls
    
    def _build_request_tasks(self, prompts, model_value, size_value, 
                             response_format_value, concurrent_requests, image_urls):
        """æ„å»ºè¯·æ±‚ä»»åŠ¡åˆ—è¡¨"""
        request_tasks = []
        for prompt_idx, current_prompt in enumerate(prompts, 1):
            payload = {
                "model": model_value,
                "prompt": current_prompt,
                "size": size_value,
                "response_format": response_format_value,
                "n": concurrent_requests,
            }
            
            if image_urls:
                if len(image_urls) == 1:
                    payload["image"] = image_urls[0]
                else:
                    payload["image"] = image_urls
            
            request_tasks.append({
                "task_id": prompt_idx,
                "prompt": current_prompt,
                "payload": payload
            })
            
            self.log(f"[DEBUG] ä»»åŠ¡ {prompt_idx} - æç¤ºè¯: {current_prompt[:50]}...", "DEBUG")
        
        return request_tasks
    
    def _send_requests(self, request_tasks, base_url, headers, timeout, max_retries):
        """å¹¶å‘å‘é€æ‰€æœ‰è¯·æ±‚"""
        print(f"\n{'='*60}")
        print(f"[INFO] å¼€å§‹å¹¶å‘å‘é€è¯·æ±‚...")
        print(f"[INFO] æç¤ºè¯æ•°é‡: {len(request_tasks)}")
        print(f"[INFO] é¢„è®¡ç”Ÿæˆå›¾ç‰‡æ•°: {sum(t['payload']['n'] for t in request_tasks)}")
        print(f"{'='*60}\n")
        
        def send_single_request(task):
            task_id = task["task_id"]
            task_prompt = task["prompt"]
            task_payload = task["payload"]
            
            print(f"\n[INFO] â–¶ ä»»åŠ¡ {task_id} å¼€å§‹è¯·æ±‚: {task_prompt[:30]}...")
            
            try:
                result = make_api_request(base_url, headers, task_payload, timeout, max_retries)
                print(f"[SUCCESS] âœ… ä»»åŠ¡ {task_id} è¯·æ±‚æˆåŠŸ")
                return {
                    "task_id": task_id,
                    "prompt": task_prompt,
                    "result": result,
                    "success": True
                }
            except Exception as e:
                print(f"[ERROR] âŒ ä»»åŠ¡ {task_id} è¯·æ±‚å¤±è´¥: {e}")
                return {
                    "task_id": task_id,
                    "prompt": task_prompt,
                    "error": str(e),
                    "success": False
                }
        
        all_results = []
        max_workers = min(len(request_tasks), 10)
        print(f"\n[INFO] ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘æ‰§è¡Œï¼Œæœ€å¤§å¹¶å‘æ•°: {max_workers}")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_task = {executor.submit(send_single_request, task): task for task in request_tasks}
            
            for future in as_completed(future_to_task):
                task_result = future.result()
                all_results.append(task_result)
        
        print(f"\n{'='*60}")
        print(f"[INFO] æ‰€æœ‰è¯·æ±‚å·²å®Œæˆï¼")
        print(f"[INFO] æˆåŠŸ: {sum(1 for r in all_results if r['success'])} ä¸ª")
        print(f"[INFO] å¤±è´¥: {sum(1 for r in all_results if not r['success'])} ä¸ª")
        print(f"{'='*60}\n")
        
        all_results.sort(key=lambda x: x["task_id"])
        return all_results
    
    def _parse_results(self, all_results, response_format_value):
        """è§£æAPIå“åº”,æ”¶é›†ä¸‹è½½ä»»åŠ¡"""
        print(f"\n{'='*60}")
        print(f"[INFO] å¼€å§‹ç»Ÿä¸€è§£æå“åº”å¹¶æ”¶é›†ä¸‹è½½ä»»åŠ¡...")
        print(f"{'='*60}\n")
        
        download_tasks = []
        
        for task_result in all_results:
            if not task_result["success"]:
                print(f"[WARN] âš  è·³è¿‡å¤±è´¥çš„ä»»åŠ¡ {task_result['task_id']}: {task_result.get('error', 'Unknown')}")
                continue
            
            result = task_result["result"]
            task_id = task_result["task_id"]
            task_prompt = task_result["prompt"]
            
            self.log(f"\n[INFO] è§£æä»»åŠ¡ {task_id} çš„å“åº” - æç¤ºè¯: {task_prompt[:30]}...", "DEBUG")
            self.log(f"[DEBUG] å“åº”åŒ…å«çš„é”®: {list(result.keys())}", "DEBUG")
            
            if "data" in result:
                data = result["data"]
                self.log(f"[DEBUG] data ç±»å‹: {type(data)}", "DEBUG")
                            
                if isinstance(data, list):
                    self.log(f"[DEBUG] data æ˜¯åˆ—è¡¨ï¼Œé•¿åº¦: {len(data)}", "DEBUG")
                    for idx, item in enumerate(data, 1):
                        self.log(f"[DEBUG] å‡†å¤‡ä¸‹è½½ä»»åŠ¡ {task_id} çš„ç¬¬ {idx}/{len(data)} å¼ å›¾ç‰‡", "DEBUG")
                        self.log(f"[DEBUG] å›¾ç‰‡é¡¹åŒ…å«çš„é”®: {list(item.keys()) if isinstance(item, dict) else 'N/A'}", "DEBUG")
                        
                        download_tasks.append({
                            "task_id": task_id,
                            "image_idx": idx,
                            "item": item,
                            "order": len(download_tasks)
                        })
                                        
                elif isinstance(data, dict):
                    self.log(f"[DEBUG] data æ˜¯å­—å…¸", "DEBUG")
                    self.log(f"[DEBUG] å­—å…¸åŒ…å«çš„é”®: {list(data.keys())}", "DEBUG")
                    
                    download_tasks.append({
                        "task_id": task_id,
                        "image_idx": 1,
                        "item": data,
                        "order": len(download_tasks)
                    })
            else:
                print(f"[ERROR] ä»»åŠ¡ {task_id} å“åº”ä¸­æ²¡æœ‰ 'data' å­—æ®µï¼")
                print(f"[DEBUG] å®Œæ•´å“åº”å†…å®¹: {result}")
                            
                if "created" in result and "usage" in result:
                    print(f"[INFO] æ£€æµ‹åˆ°å¯èƒ½æ˜¯å›¾åƒåˆ†æAPIçš„å“åº”ï¼Œæ²¡æœ‰å›¾ç‰‡æ•°æ®")
        
        return download_tasks
    
    def _download_images(self, download_tasks, response_format_value, timeout):
        """å¹¶å‘ä¸‹è½½æ‰€æœ‰å›¾ç‰‡"""
        if not download_tasks:
            raise RuntimeError("æ²¡æœ‰å¯ä¸‹è½½çš„å›¾ç‰‡ä»»åŠ¡ï¼APIå“åº”å¯èƒ½ä¸åŒ…å«å›¾ç‰‡æ•°æ®")
        
        print(f"\n{'='*60}")
        print(f"[INFO] å¼€å§‹å¹¶å‘ä¸‹è½½ {len(download_tasks)} å¼ å›¾ç‰‡...")
        print(f"{'='*60}\n")
        
        def download_single_image(task):
            task_id = task["task_id"]
            image_idx = task["image_idx"]
            item = task["item"]
            order = task["order"]
            
            print(f"[INFO] â–¶ å¼€å§‹ä¸‹è½½ä»»åŠ¡ {task_id} çš„ç¬¬ {image_idx} å¼ å›¾ç‰‡...")
            
            try:
                tensor = self._process_image_item(item, response_format_value, timeout)
                if tensor is not None:
                    print(f"[SUCCESS] âœ… ä»»åŠ¡ {task_id} ç¬¬ {image_idx} å¼ å›¾ç‰‡ä¸‹è½½æˆåŠŸ")
                    return {
                        "order": order,
                        "task_id": task_id,
                        "image_idx": image_idx,
                        "tensor": tensor,
                        "success": True
                    }
                else:
                    print(f"[ERROR] âŒ ä»»åŠ¡ {task_id} ç¬¬ {image_idx} å¼ å›¾ç‰‡ä¸‹è½½å¤±è´¥")
                    return {
                        "order": order,
                        "task_id": task_id,
                        "image_idx": image_idx,
                        "success": False
                    }
            except Exception as e:
                print(f"[ERROR] âŒ ä»»åŠ¡ {task_id} ç¬¬ {image_idx} å¼ å›¾ç‰‡ä¸‹è½½å¼‚å¸¸: {e}")
                return {
                    "order": order,
                    "task_id": task_id,
                    "image_idx": image_idx,
                    "success": False,
                    "error": str(e)
                }
        
        download_results = []
        max_download_workers = min(len(download_tasks), 4)
        print(f"[INFO] ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘ä¸‹è½½ï¼Œæœ€å¤§å¹¶å‘æ•°: {max_download_workers}\n")
        
        with ThreadPoolExecutor(max_workers=max_download_workers) as executor:
            future_to_download = {executor.submit(download_single_image, task): task for task in download_tasks}
            
            for future in as_completed(future_to_download):
                result = future.result()
                download_results.append(result)
        
        download_results.sort(key=lambda x: x["order"])
        
        output_tensors = []
        for result in download_results:
            if result["success"]:
                output_tensors.append(result["tensor"])
        
        print(f"\n{'='*60}")
        print(f"[INFO] ä¸‹è½½å®Œæˆç»Ÿè®¡")
        print(f"[INFO] æˆåŠŸ: {len(output_tensors)} å¼ ")
        print(f"[INFO] å¤±è´¥: {len(download_results) - len(output_tensors)} å¼ ")
        print(f"{'='*60}\n")
        
        # å¦‚æœæ²¡æœ‰ä¸€å¼ æˆåŠŸ,ç›´æ¥æŠ›å¼‚å¸¸(é˜²æ­¢ç¼“å­˜é”™è¯¯ç»“æœ)
        if not output_tensors:
            raise RuntimeError(f"æ‰€æœ‰å›¾ç‰‡ä¸‹è½½å‡å¤±è´¥ï¼æ€»è®¡ {len(download_results)} ä¸ªä»»åŠ¡")
        
        return output_tensors
    
    def _normalize_tensor_size(self, tensors):
        """å½’ä¸€åŒ–tensorå°ºå¯¸,é¿å…å°ºå¯¸ä¸ä¸€è‡´å¯¼è‡´stackå´©æºƒ"""
        if not tensors:
            return tensors
        
        # è·å–æ‰€æœ‰tensorçš„å°ºå¯¸
        shapes = [(t.shape[0], t.shape[1]) for t in tensors]
        heights = [s[0] for s in shapes]
        widths = [s[1] for s in shapes]
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å°ºå¯¸éƒ½ä¸€è‡´
        if len(set(shapes)) == 1:
            self.log(f"[DEBUG] æ‰€æœ‰å›¾ç‰‡å°ºå¯¸ä¸€è‡´: {shapes[0]}", "DEBUG")
            return tensors
        
        # å°ºå¯¸ä¸ä¸€è‡´,éœ€è¦å½’ä¸€åŒ–
        print(f"[WARN] âš ï¸ æ£€æµ‹åˆ°å›¾ç‰‡å°ºå¯¸ä¸ä¸€è‡´!")
        print(f"[WARN] å°ºå¯¸åˆ†å¸ƒ: {set(shapes)}")
        
        # ä½¿ç”¨æœ€å°å…¬å…±å°ºå¯¸(è£å‰ªç­–ç•¥)
        min_h = min(heights)
        min_w = min(widths)
        
        print(f"[INFO] ç»Ÿä¸€è£å‰ªåˆ°æœ€å°å…¬å…±å°ºå¯¸: {min_h}Ã—{min_w}")
        
        # ä¸­å¿ƒè£å‰ª
        normalized = []
        for idx, t in enumerate(tensors):
            h, w, c = t.shape
            
            # è®¡ç®—è£å‰ªèµ·å§‹ä½ç½®(ä¸­å¿ƒå¯¹é½)
            start_h = (h - min_h) // 2
            start_w = (w - min_w) // 2
            
            # è£å‰ª
            cropped = t[start_h:start_h+min_h, start_w:start_w+min_w, :]
            normalized.append(cropped)
            
            if h != min_h or w != min_w:
                self.log(f"[DEBUG] å›¾ç‰‡{idx+1}: {h}Ã—{w} â†’ {min_h}Ã—{min_w} (è£å‰ª)", "DEBUG")
        
        print(f"[SUCCESS] âœ… å·²å½’ä¸€åŒ– {len(normalized)} å¼ å›¾ç‰‡å°ºå¯¸")
        return normalized
    
    def _merge_tensors(self, output_tensors, prompts, concurrent_requests, enable_multiline):
        """åˆå¹¶æ‰€æœ‰tensorä¸ºæ‰¹æ¬¡"""
        if not output_tensors:
            raise RuntimeError("æœªè·å–åˆ°ä»»ä½•å›¾ç‰‡æ•°æ®ï¼")
        
        # å½’ä¸€åŒ–tensorå°ºå¯¸(é˜²æ­¢å°ºå¯¸ä¸ä¸€è‡´å¯¼è‡´stackå´©æºƒ)
        output_tensors = self._normalize_tensor_size(output_tensors)
        
        batch_tensor = torch.stack(output_tensors, dim=0).contiguous()
        print(f"\n{'='*60}")
        print(f"[SUCCESS] âœ… æˆåŠŸç”Ÿæˆ {len(output_tensors)} å¼ å›¾ç‰‡!")
        print(f"[INFO] æ‰¹æ¬¡å°ºå¯¸: {batch_tensor.shape}")
        if enable_multiline:
            print(f"[INFO] æç¤ºè¯æ•°é‡: {len(prompts)} è¡Œ")
            print(f"[INFO] æ¯è¡Œå¹¶å‘æ•°: {concurrent_requests}")
            print(f"[INFO] æ€»å›¾ç‰‡æ•°: {len(prompts)} Ã— {concurrent_requests} = {len(output_tensors)}")
        else:
            print(f"[INFO] å¹¶å‘è¯·æ±‚æ•°: {concurrent_requests}")
        print(f"{'='*60}\n")
        
        self.log(f"[DEBUG] å‡†å¤‡è¿”å› tensorï¼Œç¡®ä¿æ•°æ®å®Œæ•´æ€§...", "DEBUG")
        self.log(f"[DEBUG] tensor ç±»å‹: {type(batch_tensor)}", "DEBUG")
        self.log(f"[DEBUG] tensor device: {batch_tensor.device}", "DEBUG")
        self.log(f"[DEBUG] tensor dtype: {batch_tensor.dtype}", "DEBUG")
        
        if self.verbose:
            print(f"\n{'='*60}")
            print(f"[OUTPUT] å‡†å¤‡ä¼ é€’ç»™ä¸‹ä¸€ä¸ªèŠ‚ç‚¹çš„æ•°æ®è¯¦æƒ…:")
            print(f"{'='*60}")
            print(f"[OUTPUT] æ•°æ®ç±»å‹: {type(batch_tensor).__name__}")
            print(f"[OUTPUT] æ•°æ®å½¢çŠ¶ (shape): {batch_tensor.shape}")
            print(f"  â”œâ”€ æ‰¹æ¬¡å¤§å° (batch): {batch_tensor.shape[0]}")
            print(f"  â”œâ”€ å›¾ç‰‡é«˜åº¦ (height): {batch_tensor.shape[1]}")
            print(f"  â”œâ”€ å›¾ç‰‡å®½åº¦ (width): {batch_tensor.shape[2]}")
            print(f"  â””â”€ é€šé“æ•° (channels): {batch_tensor.shape[3]}")
            print(f"[OUTPUT] æ•°æ®ç»´åº¦ (ndim): {batch_tensor.ndim}")
            print(f"[OUTPUT] å…ƒç´ æ€»æ•°: {batch_tensor.numel():,}")
            print(f"[OUTPUT] æ•°æ®ç±»å‹ (dtype): {batch_tensor.dtype}")
            print(f"[OUTPUT] å­˜å‚¨è®¾å¤‡ (device): {batch_tensor.device}")
            print(f"[OUTPUT] æ˜¯å¦éœ€è¦æ¢¯åº¦: {batch_tensor.requires_grad}")
            print(f"[OUTPUT] å†…å­˜å¤§å°: {batch_tensor.element_size() * batch_tensor.numel() / 1024 / 1024:.2f} MB")
            print(f"[OUTPUT] æ•°å€¼èŒƒå›´: [{batch_tensor.min():.4f}, {batch_tensor.max():.4f}]")
            print(f"[OUTPUT] æ•°å€¼å‡å€¼: {batch_tensor.mean():.4f}")
            print(f"[OUTPUT] æ•°å€¼æ ‡å‡†å·®: {batch_tensor.std():.4f}")
            
            if batch_tensor.shape[0] <= 5:
                print(f"\n[OUTPUT] å„å›¾ç‰‡è¯¦ç»†ä¿¡æ¯:")
                for i in range(batch_tensor.shape[0]):
                    img_tensor = batch_tensor[i]
                    print(f"  å›¾ç‰‡ {i+1}:")
                    print(f"    â”œâ”€ å°ºå¯¸: {img_tensor.shape[1]}Ã—{img_tensor.shape[0]} ({img_tensor.shape[2]} é€šé“)")
                    print(f"    â”œâ”€ æ•°å€¼èŒƒå›´: [{img_tensor.min():.4f}, {img_tensor.max():.4f}]")
                    print(f"    â”œâ”€ å‡å€¼: {img_tensor.mean():.4f}")
                    print(f"    â””â”€ æ ‡å‡†å·®: {img_tensor.std():.4f}")
            
            print(f"\n[OUTPUT] è¿”å›å€¼ç»“æ„: tuple åŒ…å« 1 ä¸ªå…ƒç´ ")
            print(f"[OUTPUT] è¿”å›å€¼å†…å®¹: (torch.Tensor,)")
            print(f"[OUTPUT] ComfyUI å°†æ¥æ”¶åˆ°ç±»å‹ä¸º 'IMAGE' çš„è¾“å‡º")
            print(f"{'='*60}\n")
        
        return batch_tensor
    
    def generate_image(self, æç¤ºè¯, APIå¯†é’¥, APIåœ°å€, æ¨¡å‹, å®½é«˜æ¯”, 
                       å“åº”æ ¼å¼, è¶…æ—¶ç§’æ•°, æœ€å¤§é‡è¯•æ¬¡æ•°, å¹¶å‘è¯·æ±‚æ•°,
                       å¯ç”¨åˆ†è¡Œæç¤ºè¯, åŒ¹é…å‚è€ƒå°ºå¯¸, è¯¦ç»†æ—¥å¿—,
                       å‚è€ƒå›¾ç‰‡1=None, å‚è€ƒå›¾ç‰‡2=None, å‚è€ƒå›¾ç‰‡3=None, å‚è€ƒå›¾ç‰‡4=None):
        """ä¸»ç”Ÿæˆå‡½æ•° - é‡æ„ä¸ºæ¸…æ™°çš„æµç¨‹"""
        
        # è®¾ç½®æ—¥å¿—çº§åˆ«
        self.verbose = è¯¦ç»†æ—¥å¿—
        
        # é‡å‘½åå˜é‡ä»¥ä¾¿å†…éƒ¨ä½¿ç”¨
        prompt = æç¤ºè¯
        enable_multiline = å¯ç”¨åˆ†è¡Œæç¤ºè¯
        concurrent_requests = å¹¶å‘è¯·æ±‚æ•°
        match_reference_size = åŒ¹é…å‚è€ƒå°ºå¯¸
        api_key = APIå¯†é’¥
        base_url = APIåœ°å€
        model = æ¨¡å‹
        size = å®½é«˜æ¯”
        response_format = å“åº”æ ¼å¼
        timeout = è¶…æ—¶ç§’æ•°
        max_retries = æœ€å¤§é‡è¯•æ¬¡æ•°
        
        # ä¿å­˜é…ç½®åˆ°ç‹¬ç«‹é…ç½®èŠ‚ï¼ˆé‡æ–°è¯»å–ç¡®ä¿ä¸è¦†ç›–å…¶ä»–èŠ‚ç‚¹é…ç½®ï¼‰
        config_writer = configparser.ConfigParser()
        if CONFIG_PATH.exists():
            config_writer.read(CONFIG_PATH, encoding="utf-8")
        
        if not config_writer.has_section(CONFIG_SECTION):
            config_writer.add_section(CONFIG_SECTION)
        
        if api_key.strip():
            config_writer.set(CONFIG_SECTION, "api_key", api_key.strip())
        if base_url.strip():
            config_writer.set(CONFIG_SECTION, "api_url", base_url.strip())
        
        with CONFIG_PATH.open("w", encoding="utf-8") as fp:
            config_writer.write(fp)
        
        # æ‰“å°å…¶ä»–å‚æ•°
        print(f"[INFO] æ¨¡å‹: {model}")
        print(f"[INFO] å°ºå¯¸: {size}")
        print(f"[INFO] å“åº”æ ¼å¼: {response_format}")
        print(f"[INFO] è¶…æ—¶æ—¶é—´: {timeout}ç§’")
        print(f"[INFO] æœ€å¤§é‡è¯•æ¬¡æ•°: {max_retries}")
        
        # å‡†å¤‡è¯·æ±‚åŸºç¡€å‚æ•°
        model_value = MODEL_MAP[model]
        size_value = IMAGE_SIZE_MAP[size]
        response_format_value = RESPONSE_FORMAT_MAP[response_format]
        
        # å¤„ç† API åœ°å€ï¼šç¡®ä¿æ˜¯å®Œæ•´çš„ URL
        if base_url.startswith('http://') or base_url.startswith('https://'):
            # å¦‚æœå·²ç»æ˜¯å®Œæ•´ URLï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«è·¯å¾„
            if '/v1/images/generations' not in base_url:
                # åªæœ‰åŸŸåï¼Œéœ€è¦æ·»åŠ è·¯å¾„
                base_url = base_url.rstrip('/') + '/v1/images/generations'
        else:
            # å¦‚æœåªæ˜¯åŸŸåï¼Œæ·»åŠ åè®®å’Œè·¯å¾„
            base_url = f"https://{base_url}/v1/images/generations"
        
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        try:
            # æ­¥éª¤1: å‡†å¤‡æç¤ºè¯
            prompts = self._prepare_prompts(prompt, enable_multiline, concurrent_requests)
            
            # æ­¥éª¤2: å‡†å¤‡è¾“å…¥å›¾ç‰‡
            input_images = [å‚è€ƒå›¾ç‰‡1, å‚è€ƒå›¾ç‰‡2, å‚è€ƒå›¾ç‰‡3, å‚è€ƒå›¾ç‰‡4]
            input_images = [img for img in input_images if img is not None]
            image_urls = self._prepare_input_images(å‚è€ƒå›¾ç‰‡1, å‚è€ƒå›¾ç‰‡2, å‚è€ƒå›¾ç‰‡3, å‚è€ƒå›¾ç‰‡4)
            
            # æ­¥éª¤3: æ„å»ºè¯·æ±‚ä»»åŠ¡
            request_tasks = self._build_request_tasks(
                prompts, model_value, size_value, 
                response_format_value, concurrent_requests, image_urls
            )
            
            # æ­¥éª¤4: å¹¶å‘å‘é€è¯·æ±‚
            all_results = self._send_requests(request_tasks, base_url, headers, timeout, max_retries)
            
            # æ­¥éª¤5: è§£æå“åº”
            download_tasks = self._parse_results(all_results, response_format_value)
            
            # æ­¥éª¤6: ä¸‹è½½å›¾ç‰‡
            output_tensors = self._download_images(download_tasks, response_format_value, timeout)
            
            # æ­¥éª¤6.5: å¦‚æœå¯ç”¨"åŒ¹é…å‚è€ƒå°ºå¯¸"ä¸”æœ‰å‚è€ƒå›¾ç‰‡ï¼Œåˆ™è°ƒæ•´è¾“å‡ºå°ºå¯¸
            if match_reference_size and input_images:
                output_tensors = self._match_reference_size(output_tensors, input_images)
            
            # æ­¥éª¤7: åˆå¹¶ tensor
            batch_tensor = self._merge_tensors(output_tensors, prompts, concurrent_requests, enable_multiline)
            
            print(f"[INFO] âœ… èŠ‚ç‚¹æ‰§è¡Œå®Œæ¯•ï¼Œè¿”å›ç»“æœ")
            return (batch_tensor,)
            
        except Exception as e:
            # æ‰€æœ‰å¼‚å¸¸ç»Ÿä¸€å¤„ç†
            print(f"[ERROR] ç”Ÿæˆå¤±è´¥: {e}")
            print(f"[DEBUG] å¼‚å¸¸ç±»å‹: {type(e).__name__}")
            import traceback
            traceback.print_exc()
            raise
    
    def _process_image_item(self, item: dict, format_type: str, timeout: int):
        """å¤„ç†å•ä¸ªå›¾ç‰‡æ•°æ®é¡¹"""
        self.log(f"[DEBUG] _process_image_item è°ƒç”¨: format_type={format_type}", "DEBUG")
        self.log(f"[DEBUG] item å†…å®¹: {item}", "DEBUG")
        
        if format_type == "url" and "url" in item:
            self.log(f"[DEBUG] åŒ¹é…åˆ° URL æ ¼å¼ï¼Œå¼€å§‹ä¸‹è½½...", "DEBUG")
            return download_image_to_tensor(item["url"], timeout)
        elif format_type == "b64_json" and "b64_json" in item:
            self.log(f"[DEBUG] åŒ¹é…åˆ° Base64 æ ¼å¼ï¼Œå¼€å§‹è§£ç ...", "DEBUG")
            return base64_to_tensor(item["b64_json"])
        else:
            print(f"[ERROR] æœªåŒ¹é…åˆ°ä»»ä½•æ ¼å¼ï¼")
            self.log(f"[DEBUG] æœŸæœ›æ ¼å¼: {format_type}", "DEBUG")
            self.log(f"[DEBUG] item åŒ…å«çš„é”®: {list(item.keys()) if isinstance(item, dict) else 'N/A'}", "DEBUG")
            return None
    
    def _match_reference_size(self, output_tensors, input_images):
        """åŒ¹é…å‚è€ƒå›¾ç‰‡å°ºå¯¸ - ä½¿ç”¨ç¬¬ä¸€å¼ å‚è€ƒå›¾çš„å°ºå¯¸ä½œä¸ºç›®æ ‡"""
        if not output_tensors or not input_images:
            return output_tensors
        
        # è·å–ç¬¬ä¸€å¼ å‚è€ƒå›¾çš„å°ºå¯¸ (tensor shape: [H, W, C])
        ref_tensor = input_images[0]
        if len(ref_tensor.shape) > 3:
            ref_tensor = ref_tensor[0]  # å¦‚æœæ˜¯æ‰¹æ¬¡ï¼Œå–ç¬¬ä¸€å¼ 
        
        target_h = ref_tensor.shape[0]
        target_w = ref_tensor.shape[1]
        
        print(f"\n{'='*60}")
        print(f"[INFO] å¯ç”¨åŒ¹é…å‚è€ƒå°ºå¯¸åŠŸèƒ½")
        print(f"[INFO] å‚è€ƒå›¾å°ºå¯¸: {target_w}Ã—{target_h}")
        print(f"[INFO] å¾…å¤„ç†å›¾ç‰‡æ•°é‡: {len(output_tensors)}")
        print(f"{'='*60}\n")        
        matched_tensors = []
        for idx, tensor in enumerate(output_tensors):
            current_h, current_w = tensor.shape[0], tensor.shape[1]
            
            if current_h == target_h and current_w == target_w:
                self.log(f"[DEBUG] å›¾ç‰‡{idx+1} å°ºå¯¸å·²åŒ¹é…ï¼Œè·³è¿‡è°ƒæ•´", "DEBUG")
                matched_tensors.append(tensor)
            else:
                print(f"[INFO] å›¾ç‰‡{idx+1}: {current_w}Ã—{current_h} â†’ {target_w}Ã—{target_h} (ç¼©æ”¾+è£å‰ª)")                
                # è½¬æ¢ä¸º PIL Image
                array = (tensor.cpu().numpy() * 255.0).astype(np.uint8)
                pil_image = Image.fromarray(array, mode='RGB')
                
                # ä½¿ç”¨ ImageOps.fit è¿›è¡Œæ™ºèƒ½ç¼©æ”¾+å±…ä¸­è£å‰ª
                resized_image = ImageOps.fit(pil_image, (target_w, target_h), method=Image.LANCZOS)
                
                # è½¬å› tensor
                resized_array = np.array(resized_image).astype(np.float32) / 255.0
                resized_tensor = torch.from_numpy(resized_array)
                
                matched_tensors.append(resized_tensor)
        
        print(f"[SUCCESS] âœ… å·²å°† {len(matched_tensors)} å¼ å›¾ç‰‡è°ƒæ•´ä¸ºå‚è€ƒå°ºå¯¸ {target_w}Ã—{target_h}\n")
        return matched_tensors

# ComfyUI èŠ‚ç‚¹æ˜ å°„
NODE_CLASS_MAPPINGS = {
    "GeminiBananaNode": GeminiBananaNode
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "GeminiBananaNode": "artsmcp-gemini-banana"
}
